{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e4a667",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpickletools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mModel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FFNN\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fashion_mnist\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Model'"
     ]
    }
   ],
   "source": [
    "from pickletools import optimize\n",
    "import wandb\n",
    "from Model import FFNN\n",
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "# from Optimizer import momentum_gradient_descent, momentum_or_nag_gd, sgd_or_batch_gd, adam, rmsprop, reinitialize\n",
    "\n",
    "(train_x, train_Y), (test_x, test_Y) = fashion_mnist.load_data()\n",
    "\n",
    "# we have to train using the data and test for validation\n",
    "\n",
    "# def logging(data_x, data_y, val_x, val_y, epochs, batch_size, learning_rate, optimizer, layer_1, layer_2, layer_3):\n",
    "    \n",
    "#     if(optimizer == \"nag\"):\n",
    "#         ann = FFNN(learning_rate, 1) # create FFNN with all these values\n",
    "#     else:\n",
    "#         ann = FFNN(learning_rate, 0)\n",
    "#     opt = optimizer\n",
    "#     ann.add_hidden_layer(int(layer_1))\n",
    "#     ann.add_hidden_layer(int(layer_2))\n",
    "#     ann.add_hidden_layer(int(layer_3))\n",
    "#     training_accuracy = 0\n",
    "#     training_loss = 0\n",
    "#     validation_loss = 0\n",
    "#     validation_accuracy = 0\n",
    "#     for x in range(int(epochs)):\n",
    "#         for y in range(0 , len(data_x), batch_size):\n",
    "#             training_loss, training_accuracy = ann.train(1, batch_size, data_x[y : y+batch_size], data_y[y: y+batch_size], x, opt)\n",
    "#             validation_loss, validation_accuracy = ann.train(0, batch_size, val_x, val_y, x, opt)\n",
    "#         wandb.log({\"validation_accuracy\" : validation_accuracy, \"validation_loss\" : validation_loss, \"training_accuracy\" : training_accuracy, \"training_loss\" : training_loss, \"epochs\": epochs})\n",
    "\n",
    "def train(x_train, y_train):\n",
    "    init_ = wandb.init(project=\"DL_01\")\n",
    "    # Access all hyperparameter values through wandb.config\n",
    "    config = wandb.config\n",
    "    wandb.run.name = \"lr_\" + str(config.learning_rate) + \"_opt_\" + str(config.optimizer) + \"_epoch_\" + str(config.epochs) + \"_bs_\" + str(config.batch_size) + \"_act_\" + str(config.activations)\n",
    "\n",
    "    no_of_data_points = 6000\n",
    "    train_x = x_train[:no_of_data_points]\n",
    "    train_Y = y_train[:no_of_data_points]\n",
    "\n",
    "    margin = int((0.1) * len(train_x))\n",
    "\n",
    "    validation_x = train_x[:margin]\n",
    "    validation_y = train_Y[:margin]\n",
    "\n",
    "    training_x = train_x[margin:]\n",
    "    training_y = train_Y[margin:]\n",
    "\n",
    "    # logging(training_x, training_y, validation_x, validation_y, epoch=config.epochs, batch_size=config.batch_size, learning_rate=config.learning_rate, optimizer=config.optimizer, layer_1=config.layer_1, layer_2=config.layer_2, layer_3=config.layer_3)\n",
    "\n",
    "    layer_1 = int(config.layer_1)\n",
    "    layer_2 = int(config.layer_2)\n",
    "    layer_3 = int(config.layer_3)\n",
    "    optimizer = config.optimizer\n",
    "    learning_rate = float(config.learning_rate)\n",
    "    batch_size = config.batch_size\n",
    "    if(optimizer == 'sgd'):\n",
    "        batch_size = 1\n",
    "    epochs = int(config.epochs)\n",
    "    activation_function = config.activations\n",
    "    data_x = training_x / 255\n",
    "    data_y = training_y\n",
    "    val_x = validation_x / 255\n",
    "    val_y = validation_y\n",
    "    \n",
    "    # this value should be changeable\n",
    "    loss_func = 'cross_ent'\n",
    "\n",
    "    if(optimizer == \"nag\" or optimizer == 'nadam'):\n",
    "        ann = FFNN(learning_rate, 1) # create FFNN with all these values\n",
    "    else:\n",
    "        ann = FFNN(learning_rate, 0)\n",
    "    opt = optimizer\n",
    "    ann.add_hidden_layer(int(layer_1))\n",
    "    ann.add_hidden_layer(int(layer_2))\n",
    "    ann.add_hidden_layer(int(layer_3))\n",
    "    training_accuracy = 0\n",
    "    training_loss = 0\n",
    "    validation_loss = 0\n",
    "    validation_accuracy = 0\n",
    "    for x in range(int(epochs)):\n",
    "        for y in range(0 , len(data_x), batch_size):\n",
    "            training_loss, training_accuracy = ann.train(1, batch_size, data_x[y : y+batch_size], data_y[y: y+batch_size], x, opt, activation_function, loss_func)\n",
    "            validation_loss, validation_accuracy = ann.train(0, batch_size, val_x, val_y, x, opt, activation_function, loss_func)\n",
    "            wandb.log({\"validation_accuracy\" : validation_accuracy, \"validation_loss\" : validation_loss, \"training_accuracy\" : training_accuracy, \"training_loss\" : training_loss, \"epochs\": x})\n",
    "train(train_x, train_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "114b271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickletools import optimize\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c770142",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mModel\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Model'"
     ]
    }
   ],
   "source": [
    "import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c84061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
