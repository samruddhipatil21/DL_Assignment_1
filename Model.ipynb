{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d0c1ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msamruddhipatil2526\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\samruddhi\\NumPy 2024\\wandb\\run-20240314_150711-jgcdrynz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/samruddhipatil2526/DL_01/runs/jgcdrynz' target=\"_blank\">butterscotch-pie-1</a></strong> to <a href='https://wandb.ai/samruddhipatil2526/DL_01' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/samruddhipatil2526/DL_01' target=\"_blank\">https://wandb.ai/samruddhipatil2526/DL_01</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/samruddhipatil2526/DL_01/runs/jgcdrynz' target=\"_blank\">https://wandb.ai/samruddhipatil2526/DL_01/runs/jgcdrynz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "<class 'wandb.sdk.wandb_config.Config'> object has no attribute 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\wandb_config.py:162\u001b[0m, in \u001b[0;36mConfig.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(key)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ke:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\wandb_config.py:130\u001b[0m, in \u001b[0;36mConfig.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'learning_rate'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m             validation_loss, validation_accuracy \u001b[38;5;241m=\u001b[39m ann\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m0\u001b[39m, batch_size, val_x, val_y, x, opt, activation_function, loss_func)\n\u001b[0;32m     86\u001b[0m             wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m : validation_accuracy, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m : validation_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m : training_accuracy, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m : training_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: x})\n\u001b[1;32m---> 87\u001b[0m train(train_x, train_Y)\n",
      "Cell \u001b[1;32mIn[10], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(x_train, y_train)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Access all hyperparameter values through wandb.config\u001b[39;00m\n\u001b[0;32m     35\u001b[0m config \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m---> 36\u001b[0m wandb\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(config\u001b[38;5;241m.\u001b[39mlearning_rate) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_opt_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(config\u001b[38;5;241m.\u001b[39moptimizer) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_epoch_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(config\u001b[38;5;241m.\u001b[39mepochs) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_bs_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(config\u001b[38;5;241m.\u001b[39mbatch_size) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_act_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(config\u001b[38;5;241m.\u001b[39mactivations)\n\u001b[0;32m     38\u001b[0m no_of_data_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6000\u001b[39m\n\u001b[0;32m     39\u001b[0m train_x \u001b[38;5;241m=\u001b[39m x_train[:no_of_data_points]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\wandb\\sdk\\wandb_config.py:164\u001b[0m, in \u001b[0;36mConfig.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(key)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ke:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mke\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: <class 'wandb.sdk.wandb_config.Config'> object has no attribute 'learning_rate'"
     ]
    }
   ],
   "source": [
    "from pickletools import optimize\n",
    "import wandb\n",
    "#from Model import FFNN\n",
    "from keras.datasets import fashion_mnist\n",
    "import numpy as np\n",
    "# from Optimizer import momentum_gradient_descent, momentum_or_nag_gd, sgd_or_batch_gd, adam, rmsprop, reinitialize\n",
    "\n",
    "(train_x, train_Y), (test_x, test_Y) = fashion_mnist.load_data()\n",
    "\n",
    "# we have to train using the data and test for validation\n",
    "\n",
    "# def logging(data_x, data_y, val_x, val_y, epochs, batch_size, learning_rate, optimizer, layer_1, layer_2, layer_3):\n",
    "    \n",
    "#     if(optimizer == \"nag\"):\n",
    "#         ann = FFNN(learning_rate, 1) # create FFNN with all these values\n",
    "#     else:\n",
    "#         ann = FFNN(learning_rate, 0)\n",
    "#     opt = optimizer\n",
    "#     ann.add_hidden_layer(int(layer_1))\n",
    "#     ann.add_hidden_layer(int(layer_2))\n",
    "#     ann.add_hidden_layer(int(layer_3))\n",
    "#     training_accuracy = 0\n",
    "#     training_loss = 0\n",
    "#     validation_loss = 0\n",
    "#     validation_accuracy = 0\n",
    "#     for x in range(int(epochs)):\n",
    "#         for y in range(0 , len(data_x), batch_size):\n",
    "#             training_loss, training_accuracy = ann.train(1, batch_size, data_x[y : y+batch_size], data_y[y: y+batch_size], x, opt)\n",
    "#             validation_loss, validation_accuracy = ann.train(0, batch_size, val_x, val_y, x, opt)\n",
    "#         wandb.log({\"validation_accuracy\" : validation_accuracy, \"validation_loss\" : validation_loss, \"training_accuracy\" : training_accuracy, \"training_loss\" : training_loss, \"epochs\": epochs})\n",
    "\n",
    "def train(x_train, y_train):\n",
    "    init_ = wandb.init(project=\"DL_01\")\n",
    "    # Access all hyperparameter values through wandb.config\n",
    "    config = wandb.config\n",
    "    wandb.run.name = \"lr_\" + str(config.learning_rate) + \"_opt_\" + str(config.optimizer) + \"_epoch_\" + str(config.epochs) + \"_bs_\" + str(config.batch_size) + \"_act_\" + str(config.activations)\n",
    "\n",
    "    no_of_data_points = 6000\n",
    "    train_x = x_train[:no_of_data_points]\n",
    "    train_Y = y_train[:no_of_data_points]\n",
    "\n",
    "    margin = int((0.1) * len(train_x))\n",
    "\n",
    "    validation_x = train_x[:margin]\n",
    "    validation_y = train_Y[:margin]\n",
    "\n",
    "    training_x = train_x[margin:]\n",
    "    training_y = train_Y[margin:]\n",
    "\n",
    "    # logging(training_x, training_y, validation_x, validation_y, epoch=config.epochs, batch_size=config.batch_size, learning_rate=config.learning_rate, optimizer=config.optimizer, layer_1=config.layer_1, layer_2=config.layer_2, layer_3=config.layer_3)\n",
    "\n",
    "    layer_1 = int(config.layer_1)\n",
    "    layer_2 = int(config.layer_2)\n",
    "    layer_3 = int(config.layer_3)\n",
    "    optimizer = config.optimizer\n",
    "    learning_rate = float(config.learning_rate)\n",
    "    batch_size = config.batch_size\n",
    "    if(optimizer == 'sgd'):\n",
    "        batch_size = 1\n",
    "    epochs = int(config.epochs)\n",
    "    activation_function = config.activations\n",
    "    data_x = training_x / 255\n",
    "    data_y = training_y\n",
    "    val_x = validation_x / 255\n",
    "    val_y = validation_y\n",
    "    \n",
    "    # this value should be changeable\n",
    "    loss_func = 'cross_ent'\n",
    "\n",
    "    if(optimizer == \"nag\" or optimizer == 'nadam'):\n",
    "        ann = FFNN(learning_rate, 1) # create FFNN with all these values\n",
    "    else:\n",
    "        ann = FFNN(learning_rate, 0)\n",
    "    opt = optimizer\n",
    "    ann.add_hidden_layer(int(layer_1))\n",
    "    ann.add_hidden_layer(int(layer_2))\n",
    "    ann.add_hidden_layer(int(layer_3))\n",
    "    training_accuracy = 0\n",
    "    training_loss = 0\n",
    "    validation_loss = 0\n",
    "    validation_accuracy = 0\n",
    "    for x in range(int(epochs)):\n",
    "        for y in range(0 , len(data_x), batch_size):\n",
    "            training_loss, training_accuracy = ann.train(1, batch_size, data_x[y : y+batch_size], data_y[y: y+batch_size], x, opt, activation_function, loss_func)\n",
    "            validation_loss, validation_accuracy = ann.train(0, batch_size, val_x, val_y, x, opt, activation_function, loss_func)\n",
    "            wandb.log({\"validation_accuracy\" : validation_accuracy, \"validation_loss\" : validation_loss, \"training_accuracy\" : training_accuracy, \"training_loss\" : training_loss, \"epochs\": x})\n",
    "train(train_x, train_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d554f255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key='ed57c3903aa24b40dc30a68b77aad62d1489535b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8c590d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 0yux7x8l\n",
      "Sweep URL: https://wandb.ai/samruddhipatil2526/DL_01%29/sweeps/0yux7x8l\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'name' : 'sweep cross entropy',\n",
    "    'metric': {\n",
    "      'name': 'val_accuracy',\n",
    "      'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs': {\n",
    "            'values': [5,10]\n",
    "        },\n",
    "         'hidden_size':{\n",
    "            'values':[32,64,128]\n",
    "        },\n",
    "        'activation': {\n",
    "            'values': ['sigmoid','relu']\n",
    "        },\n",
    "        'loss': {\n",
    "            'values': ['cross_entropy']\n",
    "        },\n",
    "\n",
    "\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project='DL_01)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae4103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce18138d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74583217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from Optimizer import momentum_gradient_descent, momentum_or_nag_gd, sgd_or_batch_gd, rmsprop, adam\n",
    "\n",
    "def cross_entropy(out):\n",
    "    loss = -1 *  np.log(out + 0.0001)\n",
    "    return loss\n",
    "\n",
    "def squared_error(y_true, y_pred) :\n",
    "    loss=0\n",
    "    l=len(y_true)\n",
    "    for i in range (l):\n",
    "     loss+= np.square(y_true[i]-y_pred[i])/l\n",
    "    return loss\n",
    "\n",
    "def Sigmoid(x): # here input is a vector\n",
    "    return 1 / (1 + (np.e) ** (-x)) \n",
    "\n",
    "def exp (y) :\n",
    "\treturn (np.e)**y\n",
    "\n",
    "def Softmax(x):\n",
    "    return exp(x) / np.sum(exp(x))\n",
    "\n",
    "def Diff_Sigmoid(x) :\n",
    "    return np.multiply(Sigmoid(x), ( np.ones(x.shape) - Sigmoid(x) ))\n",
    "\n",
    "def Reshape (vector):\n",
    "    return vector.reshape(vector.shape[0],1)\n",
    "\n",
    "def Relu(x) :\n",
    "    x= x / np.max(x)\n",
    "    y= np.maximum(0,x)\n",
    "    #print (y)\n",
    "    return y\n",
    "\n",
    "def Diff_Relu (x):\n",
    "    y= np.zeros(x.shape)\n",
    "    for i in range (len (x)): \n",
    "        y[i]=1 if x[i]>=0 else 0\n",
    "    return y \n",
    "\n",
    "def Tanh (x):\n",
    "    return (exp(x) - exp (-x))/(exp(x) + exp (-x))\n",
    "\n",
    "def Diff_Tanh (x) :\n",
    "    return (np.ones(x.shape) - np.square(Tanh(x)))\n",
    "\n",
    "class Output_Layer:\n",
    "    def __init__(self, neurons, prev_layer_neurons):\n",
    "        self.neurons = neurons\n",
    "        self.weights = np.random.rand(neurons, prev_layer_neurons) - 0.5\n",
    "        self.biases = np.random.rand(neurons) - 0.5\n",
    "        self.g_weights = np.zeros(self.weights.shape)\n",
    "        self.g_biases = np.zeros(self.biases.shape)\n",
    "         # added for momentum gradient descent\n",
    "        self. prev_v_w= np.zeros(self.weights.shape)\n",
    "        self.prev_v_b=  np.zeros(self.biases.shape)\n",
    "        #added for adam\n",
    "        self.m_w= np.zeros(self.weights.shape)\n",
    "        self.m_b=  np.zeros(self.biases.shape)\n",
    "    \n",
    "    def change_weights(self, prev_layer_neurons):\n",
    "        self.weights = np.random.rand(self.neurons, prev_layer_neurons)\n",
    "        self.g_weights = np.zeros(self.weights.shape)\n",
    "        self. prev_v_w= np.zeros(self.weights.shape)\n",
    "        self.m_w= np.zeros(self.weights.shape)\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.pre_activation = np.dot(self.weights, input_) + self.biases\n",
    "        self.post_activation = Softmax(self.pre_activation)\n",
    "        return self.post_activation\n",
    "    \n",
    "    def backward(self, output_true, output_pred, prev_post_activation, prev_pre_activation,gamma,isnag, post_activation_function, loss_function='cross_ent'):\n",
    "        if(loss_function == \"cross_ent\"):\n",
    "            self.grad_a_Ltheta = -(np.subtract(output_true, output_pred))\n",
    "        if(loss_function == \"sq_err\"):\n",
    "            one=np.ones(output_pred.shape)\n",
    "            self.grad_a_Ltheta =-(output_true- output_pred)*(output_pred)*(one-output_pred)\n",
    "        #print(\"True=\",output_true)\n",
    "        #print(\"Pred=\",output_pred)\n",
    "        #print(\"grad A\", self.grad_a_Ltheta)\n",
    "        # print(\"grad A\", prev_post_activation)\n",
    "        self.grad_W_Ltheta = np.matmul(Reshape(self.grad_a_Ltheta), np.transpose(Reshape(prev_post_activation)))\n",
    "        #print(\"grad W_output=\", self.grad_W_Ltheta)\n",
    "        self.grad_b_Ltheta = self.grad_a_Ltheta\n",
    "        #print(\"grad B out_put=\", self.grad_b_Ltheta)\n",
    "        weight= self.weights-gamma*isnag*self.prev_v_w \n",
    "        #print(\"Output Layer\",self.weights)\n",
    "        #print(\"Output Layer\",weight)\n",
    "        self.grad_prev_post_activation_Ltheta = np.ndarray.flatten(np.matmul(np.transpose(weight), Reshape(self.grad_a_Ltheta)))\n",
    "        if post_activation_function==\"relu\" :\n",
    "            self.dg=Diff_Relu(prev_pre_activation)\n",
    "        if post_activation_function==\"logistic\" :\n",
    "            self.dg = Diff_Sigmoid(prev_pre_activation)\n",
    "        elif post_activation_function==\"tanh\":\n",
    "            self.dg = Diff_Tanh(prev_pre_activation)\n",
    "        # print(\"grad_hk-1=\",self.grad_prev_post_activation_Ltheta)\n",
    "        # print(\"g'(a)=\",self.dg)\n",
    "        self.grad_a_Ltheta = np.ndarray.flatten(np.multiply(Reshape(self.grad_prev_post_activation_Ltheta), Reshape(self.dg)))\n",
    "        return self.grad_a_Ltheta\n",
    "\n",
    "\n",
    "class Hidden_Layer:\n",
    "    def __init__(self, neurons, prev_layer_neurons):\n",
    "        self.neurons = neurons\n",
    "        self.weights = np.random.rand(neurons, prev_layer_neurons) - 0.5\n",
    "        self.biases = np.random.rand(neurons) - 0.5\n",
    "        self.g_weights = np.zeros(self.weights.shape)\n",
    "        self.g_biases = np.zeros(self.biases.shape)\n",
    "        # added for momentum gradient descent\n",
    "        self. prev_v_w= np.zeros(self.weights.shape)\n",
    "        self.prev_v_b=  np.zeros(self.biases.shape)\n",
    "        # added for adam\n",
    "        self.m_w= np.zeros(self.weights.shape)\n",
    "        self.m_b=  np.zeros(self.biases.shape)\n",
    "\n",
    "    def forward(self, input_, post_activation_function):\n",
    "        self.pre_activation = np.dot(self.weights, input_) + self.biases\n",
    "        if post_activation_function==\"relu\":\n",
    "            self.post_activation= Relu(self.pre_activation)\n",
    "        elif post_activation_function==\"logistic\":\n",
    "            self.post_activation = Sigmoid(self.pre_activation)\n",
    "        elif post_activation_function==\"tanh\":\n",
    "            self.post_activation = Tanh(self.pre_activation)\n",
    "        return self.post_activation\n",
    "\n",
    "    def backward(self, grad_a_LTheta, prev_pre_activation, prev_post_activation,gamma,isnag, post_activation_function):\n",
    "        \n",
    "        self.grad_W_Ltheta = np.matmul(Reshape(grad_a_LTheta), np.transpose(Reshape(prev_post_activation)))\n",
    "        #print(\"grad G=\",self.grad_W_Ltheta)\n",
    "        self.grad_b_Ltheta = grad_a_LTheta\n",
    "        #print(\"gradB\",self.grad_b_Ltheta)\n",
    "        # print(\"h\", prev_post_activation)\n",
    "        weight= self.weights-gamma*isnag*self.prev_v_w\n",
    "        #print(\"Hidden Layer\",gamma*isnag*self. prev_v_w*1000)\n",
    "        self.grad_prev_post_activation_Ltheta= np.ndarray.flatten(np.dot(np.transpose(weight), Reshape(grad_a_LTheta)))\n",
    "        if post_activation_function==\"relu\" :\n",
    "            self.dg=Diff_Relu(prev_pre_activation)\n",
    "        if post_activation_function==\"logistic\" :\n",
    "            self.dg = Diff_Sigmoid(prev_pre_activation)\n",
    "        elif post_activation_function==\"tanh\":\n",
    "            self.dg= Diff_Tanh(prev_pre_activation)\n",
    "        # print(\"grad_hk-1=\",self.grad_prev_post_activation_Ltheta)\n",
    "        # print(\"g'(a)=\",self.dg)\n",
    "        self.prev_grad_a_Ltheta = np.ndarray.flatten(np.multiply(Reshape(self.grad_prev_post_activation_Ltheta), Reshape(self.dg)))\n",
    "        return self.prev_grad_a_Ltheta\n",
    "\n",
    "# prev_grad_a_Ltheta refers to the i+1 th layer gradient whereas prev_post_activation and prev_pre_activation refers to i-1 th layer pre and post activation fns\n",
    "        \n",
    "class FFNN:\n",
    "    def __init__(self, learning_rate, var=0): # the arguments for passing data are not required\n",
    "        # self.test_x = test_x\n",
    "        # self.test_y = test_y\n",
    "        # self.real_outputs = outputs_\n",
    "        # self.input_layer = input_ # this is the input vector\n",
    "        self.ip_dim = 784\n",
    "        self.layers = []\n",
    "        self.add_default_layers(self.ip_dim, 10)\n",
    "        self.eta = learning_rate\n",
    "        self.gamma= 0.9\n",
    "        self.isnag= var\n",
    "        self.eps=1e-8\n",
    "        self.beta1=0.9\n",
    "        self.beta2=0.999\n",
    "\n",
    "    def add_default_layers(self, input_dim, output_dim):\n",
    "        self.output_layer = Output_Layer(output_dim, input_dim)\n",
    "\n",
    "    def add_hidden_layer(self, no_of_neurons):\n",
    "        if(len(self.layers) == 0):\n",
    "            new_layer = Hidden_Layer(no_of_neurons, self.ip_dim)\n",
    "        else:\n",
    "            new_layer = Hidden_Layer(no_of_neurons, self.layers[-1].neurons)\n",
    "        self.layers.append(new_layer)\n",
    "        # changing the weights of output layer each time a new hidden layer is added\n",
    "        self.output_layer.change_weights(self.layers[-1].neurons)\n",
    "    def forwardprop(self,activation, post_activation_function):\n",
    "        \n",
    "        for x in range(len(self.layers)):\n",
    "            activation = self.layers[x].forward(activation, post_activation_function)\n",
    "        op_ = self.output_layer.forward(activation)\n",
    "        return op_\n",
    "    def backprop (self,op,img,true_y, batchsize, post_activation_function, loss_function):\n",
    "        self.prev_post_activation = self.layers[-1].post_activation\n",
    "        self.prev_pre_activation = self.layers[-1].pre_activation          \n",
    "        self.grad_a_Ltheta = self.output_layer.backward(true_y, op, self.prev_post_activation, self.prev_pre_activation,self.gamma,self.isnag, post_activation_function, loss_function)\n",
    "        self.output_layer.g_weights += (1/ batchsize) * self.output_layer.grad_W_Ltheta # (10, 100)\n",
    "        #print(\"Dw_sum=\",self.output_layer.g_weights)\n",
    "        self.output_layer.g_biases += (1/ batchsize) * self.output_layer.grad_b_Ltheta # (10, )\n",
    "\n",
    "        for x in range(len(self.layers)-1,-1,-1):\n",
    "            #print(\"Hidden Layer=\",x)\n",
    "            if x==0:\n",
    "                post_activation = np.ndarray.flatten(img)\n",
    "                pre_activation = np.zeros(post_activation.shape)\n",
    "            else:\n",
    "                pre_activation = self.layers[x-1].pre_activation # previous layer a(k-1) \n",
    "                post_activation = self.layers[x-1].post_activation # previous layer h(k-1)\n",
    "                \n",
    "            self.grad_a_Ltheta = self.layers[x].backward(self.grad_a_Ltheta, pre_activation, post_activation,self.gamma,self.isnag, post_activation_function)\n",
    "            self.layers[x].g_weights += (1/ batchsize) * self.layers[x].grad_W_Ltheta              \n",
    "            self.layers[x].g_biases += (1/ batchsize) * self.layers[x].grad_b_Ltheta\n",
    "        \n",
    "    # average over all the gradients by passing the batch size\n",
    "    \n",
    "    def train(self, train_, batch_size, data_x, data_y, epoch, opt='sgd', activation_func='logistic', loss_function='cross_ent'):\n",
    "        i = 0\n",
    "        count = 0\n",
    "        avg_loss=0\n",
    "        for img in data_x:\n",
    "            true_y = np.zeros(10)\n",
    "            true_y[data_y[i]] = 1\n",
    "            activation = np.ndarray.flatten(img)   \n",
    "            op = self.forwardprop(activation, activation_func)\n",
    "            if(loss_function == 'cross_ent'):\n",
    "                avg_loss += cross_entropy(op[data_y[i]])\n",
    "            if(loss_function == 'sq_err'):\n",
    "                avg_loss +=  squared_error(true_y, op)\n",
    "            if(np.argmax(op) == data_y[i]):\n",
    "                count += 1\n",
    "            i += 1\n",
    "            if(train_):\n",
    "                self.backprop(op,img,true_y, batch_size, activation_func, loss_function) # computing the gradients for each image and adding them up\n",
    "        acc = count / len(data_x)\n",
    "        avg_loss = avg_loss / len(data_x)\n",
    "        if(train_):\n",
    "            if(opt == 'momentum'):\n",
    "                self = momentum_gradient_descent(self)\n",
    "            if(opt == 'sgd'):\n",
    "                self = sgd_or_batch_gd(self, batch_size)\n",
    "            if(opt == 'rmsprop'):\n",
    "                self = rmsprop(self)\n",
    "            if(opt == 'adam'):\n",
    "                self = adam(self, epoch)\n",
    "            if(opt == 'nag'):\n",
    "                self = momentum_or_nag_gd(self)\n",
    "            if(opt == 'nadam'):\n",
    "                self = adam(self, epoch)\n",
    "            self.reinitialize()\n",
    "        return avg_loss, acc\n",
    "    \n",
    "    def reinitialize(self):\n",
    "        self.output_layer.g_weights = np.zeros(self.output_layer.weights.shape)\n",
    "        self.output_layer.g_biases = np.zeros(self.output_layer.biases.shape)\n",
    "        for x in self.layers:\n",
    "            x.g_biases = np.zeros(x.biases.shape)\n",
    "            x.g_weights = np.zeros(x.weights.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe9d9faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\samruddhi\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#import Model \n",
    "#import numpy as np\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "def momentum_gradient_descent(ann):\n",
    "    v_w= ann.gamma * ann.output_layer.prev_v_w + ann.eta * ann.output_layer.g_weights\n",
    "    ann.output_layer.weights -= v_w\n",
    "    v_b=  ann.gamma * ann.output_layer.prev_v_b + ann.eta * ann.output_layer.g_biases\n",
    "    ann.output_layer.biases -= v_b\n",
    "    ann.output_layer.prev_v_w =v_w\n",
    "    ann.output_layer.prev_v_b= v_b\n",
    "    for x in (ann.layers):\n",
    "        v_w= ann.gamma * x.prev_v_w + ann.eta * x.g_weights\n",
    "        #print(\"GRAD G\",x.g_weights)\n",
    "        x.weights -= v_w\n",
    "        v_b=  ann.gamma * x.prev_v_b + ann.eta * x.g_biases\n",
    "        x.biases -= v_b\n",
    "        x.prev_v_w=v_w\n",
    "        #print(\"update\",x.prev_v_w)\n",
    "        x.prev_v_b=v_b\n",
    "    return ann\n",
    "\n",
    "def momentum_or_nag_gd(ann):\n",
    "    v_w= ann.gamma * ann.output_layer.prev_v_w + ann.eta * ann.output_layer.g_weights\n",
    "    ann.output_layer.weights -= v_w\n",
    "    v_b=  ann.gamma * ann.output_layer.prev_v_b + ann.eta * ann.output_layer.g_biases\n",
    "    ann.output_layer.biases -= v_b\n",
    "    ann.output_layer.prev_v_w =v_w\n",
    "    ann.output_layer.prev_v_b= v_b\n",
    "    for x in (ann.layers):\n",
    "        v_w= ann.gamma * x.prev_v_w + ann.eta * x.g_weights\n",
    "        x.weights -= v_w\n",
    "        v_b=  ann.gamma * x.prev_v_b + ann.eta * x.g_biases\n",
    "        x.biases -= v_b\n",
    "        x.prev_v_w=v_w\n",
    "        x.prev_v_b=v_b\n",
    "    return ann\n",
    "\n",
    "def sgd_or_batch_gd(ann, n):\n",
    "    ann.output_layer.weights -= (ann.eta * ann.output_layer.g_weights)\n",
    "    ann.output_layer.biases -= (ann.eta * ann.output_layer.g_biases)\n",
    "    for x in (ann.layers):\n",
    "        x.weights -= (ann.eta * x.g_weights)\n",
    "        x.biases -= (ann.eta * x.g_biases)\n",
    "    return ann\n",
    "\n",
    "def rmsprop(ann):\n",
    "    \n",
    "    ann.output_layer.prev_v_w= ann.beta1 * ann.output_layer.prev_v_w + (1-ann.beta1) * (ann.output_layer.g_weights ** 2)\n",
    "    ann.output_layer.weights -= (ann.eta/np.sqrt(ann.output_layer.prev_v_w + ann.eps))* ann.output_layer.g_weights\n",
    "    ann.output_layer.prev_v_b=  ann.beta1 * ann.output_layer.prev_v_b + (1-ann.beta1) *  (ann.output_layer.g_biases ** 2)\n",
    "    ann.output_layer.biases -= (ann.eta/np.sqrt(ann.output_layer.prev_v_b + ann.eps))* ann.output_layer.g_biases\n",
    "    for x in (ann.layers):\n",
    "        x.prev_v_w= ann.beta1 * x.prev_v_w + (1-ann.beta1) * (x.g_weights ** 2)\n",
    "        x.weights -= (ann.eta/np.sqrt(x.prev_v_w + ann.eps))* x.g_weights\n",
    "        x.prev_v_b= ann.beta1 * x.prev_v_b + (1-ann.beta1) * (x.g_biases** 2)\n",
    "        x.biases -= (ann.eta/np.sqrt(x.prev_v_b + ann.eps))* x.g_biases\n",
    "    return ann\n",
    "\n",
    "def adam(ann, epoch):\n",
    "    ann.output_layer.m_w= ann.beta1*ann.output_layer.m_w + (1-ann.beta1)* ann.output_layer.g_weights\n",
    "    ann.output_layer.m_b= ann.beta1*ann.output_layer.m_b + (1-ann.beta1)* ann.output_layer.g_biases\n",
    "    \n",
    "    ann.output_layer.prev_v_w=  ann.beta2*ann.output_layer.prev_v_w+ (1-ann.beta2)* (ann.output_layer.g_weights **2)\n",
    "    ann.output_layer.prev_v_b=  ann.beta2*ann.output_layer.prev_v_b+ (1-ann.beta2)* (ann.output_layer.g_biases **2)\n",
    "\n",
    "    m_w_hat=ann.output_layer.m_w/(1- (ann.beta1 ** (epoch+1)))\n",
    "    m_b_hat=ann.output_layer.m_b/(1- (ann.beta1 ** (epoch+1)))\n",
    "\n",
    "    v_w_hat= ann.output_layer.prev_v_w/(1- (ann.beta2 ** (epoch+1)))\n",
    "    v_b_hat= ann.output_layer.prev_v_b/(1- (ann.beta2 ** (epoch+1)))\n",
    "\n",
    "    ann.output_layer.weights -= (ann.eta/np.sqrt(v_w_hat + ann.eps))* m_w_hat\n",
    "    ann.output_layer.biases -= (ann.eta/np.sqrt(v_b_hat + ann.eps))* m_b_hat\n",
    "\n",
    "    for x in (ann.layers):\n",
    "        x.m_w= ann.beta1*x.m_w + (1-ann.beta1)* x.g_weights\n",
    "        x.m_b= ann.beta1*x.m_b + (1-ann.beta1)* x.g_biases\n",
    "\n",
    "        x.prev_v_w=  ann.beta2*x.prev_v_w+ (1-ann.beta2)* (x.g_weights **2)\n",
    "        x.prev_v_b=  ann.beta2*x.prev_v_b+ (1-ann.beta2)* (x.g_biases **2)\n",
    "\n",
    "        m_w_hat=x.m_w/(1- (ann.beta1 ** (epoch+1)))\n",
    "        m_b_hat=x.m_b/(1- (ann.beta1 ** (epoch+1)))\n",
    "\n",
    "        v_w_hat= x.prev_v_w/(1- (ann.beta2 ** (epoch+1)))\n",
    "        v_b_hat= x.prev_v_b/(1- (ann.beta2 ** (epoch+1)))\n",
    "\n",
    "        x.weights -= (ann.eta/np.sqrt(v_w_hat + ann.eps))* m_w_hat\n",
    "        x.biases -= (ann.eta/np.sqrt(v_b_hat + ann.eps))* m_b_hat\n",
    "    return ann\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
